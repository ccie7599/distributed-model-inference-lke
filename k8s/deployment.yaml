apiVersion: apps/v1
kind: Deployment
metadata:
  name: bert-inference
  namespace: bert-inference
  labels:
    app: bert-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bert-inference
  template:
    metadata:
      labels:
        app: bert-inference
    spec:
      # Schedule on GPU nodes
      nodeSelector:
        lke.linode.com/pool-id: gpu

      # Ensure pods don't get scheduled on nodes without GPUs
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

      containers:
        - name: bert-inference
          # Using Microsoft's ONNX Runtime GPU image as base
          image: mcr.microsoft.com/onnxruntime/server:latest
          imagePullPolicy: Always

          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 50051
              protocol: TCP

          envFrom:
            - configMapRef:
                name: bert-inference-config

          resources:
            requests:
              memory: "4Gi"
              cpu: "2"
              nvidia.com/gpu: "1"
            limits:
              memory: "8Gi"
              cpu: "4"
              nvidia.com/gpu: "1"

          volumeMounts:
            - name: model-storage
              mountPath: /models

          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: bert-model-storage

      # Graceful shutdown
      terminationGracePeriodSeconds: 30
